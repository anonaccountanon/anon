{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y65niXUmMQxO"
      },
      "outputs": [],
      "source": [
        "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np, random, os, time, tarfile, gc\n",
        "import copy\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device\", device)\n",
        "torch.manual_seed(1); random.seed(1); np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WwhsojhMTXP"
      },
      "outputs": [],
      "source": [
        "class PermutedMNIST:\n",
        "    def __init__(self, num_tasks=10, seed=123):\n",
        "        shuffler = torch.Generator().manual_seed(seed)\n",
        "        self.perms = [torch.randperm(784, generator=shuffler) for _ in range(num_tasks)]\n",
        "        tfm = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "        tr = datasets.MNIST(\"./data\", True,  download=True, transform=tfm)\n",
        "        te = datasets.MNIST(\"./data\", False, download=True, transform=tfm)\n",
        "        x_tr = tr.data.float().view(-1,784)/255.\n",
        "        x_te = te.data.float().view(-1,784)/255.\n",
        "        y_tr = F.one_hot(tr.targets, 10).float()\n",
        "        y_te = F.one_hot(te.targets, 10).float()\n",
        "        self.tasks = [(TensorDataset(x_tr[:, p], y_tr), TensorDataset(x_te[:, p], y_te))for p in self.perms]\n",
        "        self.input_dim = 784\n",
        "        self.n_classes = 10\n",
        "        self.num_tasks = num_tasks\n",
        "    def get_task(self, tid):\n",
        "        return self.tasks[tid]\n",
        "\n",
        "class SplitMNIST:\n",
        "    pairs = [(0,1), (2,3), (4,5), (6,7), (8,9)]\n",
        "    def __init__(self):\n",
        "        tfm = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "        tr = datasets.MNIST(\"./data\", True,  download=True, transform=tfm)\n",
        "        te = datasets.MNIST(\"./data\", False, download=True, transform=tfm)\n",
        "        x_tr = tr.data.float().view(-1,784)/255.\n",
        "        x_te = te.data.float().view(-1,784)/255.\n",
        "        self.tasks=[]\n",
        "        for a, b in self.pairs:\n",
        "            msk_tr = (tr.targets==a)|(tr.targets==b)\n",
        "            msk_te = (te.targets==a)|(te.targets==b)\n",
        "            y_tr = F.one_hot((tr.targets[msk_tr]==b).long(), 2).float()\n",
        "            y_te = F.one_hot((te.targets[msk_te]==b).long(), 2).float()\n",
        "            self.tasks.append((TensorDataset(x_tr[msk_tr], y_tr), TensorDataset(x_te[msk_te], y_te)))\n",
        "        self.input_dim = 784\n",
        "        self.n_classes = 2\n",
        "        self.num_tasks = 5\n",
        "    def get_task(self, tid):\n",
        "        return self.tasks[tid]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVQuXfM8MbE-"
      },
      "outputs": [],
      "source": [
        "class SplitNotMNIST:\n",
        "    # pairs are A/F , B/G , C/H , D/I , E/J: A, B, C, D, E, F, G, H, I, J\n",
        "    _pairs = [(0,5), (1,6), (2,7), (3,8), (4,9)]\n",
        "    def __init__(self, path=\"notMNIST_small.tar.gz\", num_tasks=5, seed=0):\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        base_folder = os.path.splitext(os.path.splitext(path)[0])[0]\n",
        "        if not os.path.isdir(base_folder):\n",
        "            with tarfile.open(path, \"r:gz\") as tar:\n",
        "                tar.extractall()\n",
        "\n",
        "        xs, ys = [], []\n",
        "        char_to_idx = {chr(ord('A')+i): i for i in range(10)}\n",
        "        for char, idx in char_to_idx.items():\n",
        "            folder = os.path.join(base_folder, char)\n",
        "            for fname in os.listdir(folder):\n",
        "                if fname.startswith('.'): continue\n",
        "                path = os.path.join(folder, fname)\n",
        "                try:\n",
        "                    img = Image.open(path).convert(\"L\")\n",
        "                    arr = np.asarray(img, dtype=np.float32)\n",
        "                    arr /= 255.0\n",
        "                    if arr.shape == (28,28):\n",
        "                        xs.append(arr.flatten())\n",
        "                        ys.append(idx)\n",
        "                except Exception:\n",
        "                    pass\n",
        "        xs = torch.tensor(np.stack(xs))\n",
        "        ys = torch.tensor(ys)\n",
        "\n",
        "        # do te/tr div\n",
        "        N = xs.size(0)\n",
        "        perm = torch.randperm(N, generator=torch.Generator().manual_seed(seed))\n",
        "        split = int(0.9*N)\n",
        "        x_tr, y_tr = xs[perm[:split]], ys[perm[:split]]\n",
        "        x_te, y_te = xs[perm[split:]], ys[perm[split:]]\n",
        "        del xs, ys; gc.collect()\n",
        "\n",
        "        self.tasks = []\n",
        "        for tid, pair in enumerate(self._pairs[:num_tasks]):\n",
        "            a,b      = pair\n",
        "            msk_tr   = (y_tr==a)|(y_tr==b)\n",
        "            msk_te   = (y_te==a)|(y_te==b)\n",
        "            map_helper = {a:0, b:1}\n",
        "            ytr_m = torch.tensor([map_helper[l.item()] for l in y_tr[msk_tr]])\n",
        "            yte_m = torch.tensor([map_helper[l.item()] for l in y_te[msk_te]])\n",
        "            ytr_1h = F.one_hot(ytr_m, 2).float()\n",
        "            yte_1h = F.one_hot(yte_m, 2).float()\n",
        "            self.tasks.append((TensorDataset(x_tr[msk_tr], ytr_1h), TensorDataset(x_te[msk_te], yte_1h)))\n",
        "\n",
        "        self.input_dim = 784\n",
        "        self.n_classes = 2\n",
        "\n",
        "    def get_task(self, tid):\n",
        "        return self.tasks[tid]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkvtRvFOMdm8"
      },
      "outputs": [],
      "source": [
        "class PlainMLP(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super().__init__()\n",
        "        layers=[]\n",
        "        for din,dout in zip(dims[:-1],dims[1:]):\n",
        "            layers.append(nn.Linear(din,dout))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.pop() # last one no relu\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "class BayesianLinear(nn.Module):\n",
        "    def __init__(self, in_f, out_f, prior_var=1.0):\n",
        "        super().__init__()\n",
        "        self.w_mu     = nn.Parameter(torch.empty(out_f,in_f))\n",
        "        self.w_logvar = nn.Parameter(torch.full((out_f,in_f), -6.0))\n",
        "        self.b_mu     = nn.Parameter(torch.empty(out_f))\n",
        "        self.b_logvar = nn.Parameter(torch.full((out_f,), -6.0))\n",
        "        nn.init.normal_(self.w_mu,0,0.1); nn.init.normal_(self.b_mu,0,0.1)\n",
        "        self.register_buffer(\"pw_mu\", torch.zeros_like(self.w_mu))\n",
        "        self.register_buffer(\"pw_logvar\", torch.full_like(self.w_mu, math.log(prior_var)))\n",
        "        self.register_buffer(\"pb_mu\", torch.zeros_like(self.b_mu))\n",
        "        self.register_buffer(\"pb_logvar\", torch.full_like(self.b_mu, math.log(prior_var)))\n",
        "    def sample(self):\n",
        "        ew = torch.randn_like(self.w_mu)\n",
        "        eb = torch.randn_like(self.b_mu)\n",
        "        w = self.w_mu + (0.5*self.w_logvar).exp()*ew\n",
        "        b = self.b_mu + (0.5*self.b_logvar).exp()*eb\n",
        "        return w,b\n",
        "    def forward(self,x,sample=True):\n",
        "        w,b = self.sample() if sample else (self.w_mu, self.b_mu)\n",
        "        return F.linear(x,w,b)\n",
        "    def helper_kl(self, m, lv, m0, lv0):\n",
        "        v, v0 = lv.exp(), lv0.exp()\n",
        "        return 0.5*((lv0-lv) + (v+(m-m0).pow(2))/v0 -1).sum()\n",
        "    def kl(self):\n",
        "        return self.helper_kl(self.w_mu,self.w_logvar,self.pw_mu,self.pw_logvar) + self.helper_kl(self.b_mu,self.b_logvar,self.pb_mu,self.pb_logvar)\n",
        "    def update_prior(self):\n",
        "        self.pw_mu.data.copy_(self.w_mu.data)\n",
        "        self.pw_logvar.data.copy_(self.w_logvar.data)\n",
        "        self.pb_mu.data.copy_(self.b_mu.data)\n",
        "        self.pb_logvar.data.copy_(self.b_logvar.data)\n",
        "\n",
        "class BayesianMLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, out_dim, heads=1, prior_var=1.0):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.ModuleList()\n",
        "        last = in_dim\n",
        "        for h in hidden:\n",
        "            self.hidden.append(BayesianLinear(last,h,prior_var))\n",
        "            last = h\n",
        "        self.heads = nn.ModuleList([BayesianLinear(last,out_dim,prior_var) for _ in range(heads)])\n",
        "        self.out_dim = out_dim\n",
        "    def add_head(self, out_dim):\n",
        "        head = BayesianLinear(self.hidden[-1].w_mu.size(0), out_dim)\n",
        "        head.to(next(self.parameters()).device); self.heads.append(head)\n",
        "    def forward(self,x,head_id=0,sample=True):\n",
        "        for l in self.hidden: x = torch.relu(l(x,sample))\n",
        "        return self.heads[head_id](x,sample)\n",
        "    def kl(self):\n",
        "        return sum(l.kl() for l in self.hidden)+sum(h.kl() for h in self.heads)\n",
        "    def update_prior(self):\n",
        "        for l in self.hidden: l.update_prior()\n",
        "        for h in self.heads:  h.update_prior()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMJtsjJiMh0R"
      },
      "outputs": [],
      "source": [
        "class VCL:\n",
        "    def __init__(self, in_dim, hidden, n_classes,\n",
        "                 single_head=True, lr=1e-3, mc=10,\n",
        "                 prior_var=1.0, coreset_size=0,\n",
        "                 coreset_method=\"random\",\n",
        "                 coreset_epochs=20):\n",
        "\n",
        "        self.in_dim, self.hidden_sizes = in_dim, list(hidden)\n",
        "        self.n_classes = n_classes\n",
        "        self.single_head = single_head\n",
        "        self.lr, self.mc = lr, mc\n",
        "        self.prior_var = prior_var\n",
        "        self.coreset_size = coreset_size\n",
        "        self.coreset_method = coreset_method\n",
        "        self.coreset_epochs = coreset_epochs\n",
        "\n",
        "\n",
        "        init_heads = 1\n",
        "        self.model = BayesianMLP(in_dim=self.in_dim, hidden=self.hidden_sizes, out_dim=self.n_classes, heads=init_heads, prior_var=self.prior_var).to(device)\n",
        "        self.acc_hist = []\n",
        "        self.core_x, self.core_y = [], []\n",
        "\n",
        "        print(f\"VCLTrainer: Single Head: {self.single_head}, Coreset Size: {self.coreset_size}, coreset_method = {self.coreset_method}\")\n",
        "\n",
        "    def coreset_selection(self, x_full, y_full):\n",
        "        n_samples = x_full.size(0)\n",
        "        if self.coreset_size <= 0:\n",
        "             return None, None, x_full, y_full\n",
        "        if self.coreset_size >= n_samples:\n",
        "             print(\"Coreset size > train size\")\n",
        "             return x_full, y_full, None, None\n",
        "\n",
        "        if self.coreset_method == \"random\":\n",
        "            perm = torch.randperm(n_samples, device=x_full.device)\n",
        "            core_idx = perm[:self.coreset_size]\n",
        "            non_core_mask = torch.ones(n_samples, dtype=torch.bool, device=x_full.device)\n",
        "            non_core_mask[core_idx] = False\n",
        "\n",
        "            core_x, core_y = x_full[core_idx], y_full[core_idx]\n",
        "            non_core_x, non_core_y = x_full[non_core_mask], y_full[non_core_mask]\n",
        "\n",
        "            return core_x, core_y, non_core_x, non_core_y\n",
        "\n",
        "        elif self.coreset_method == \"k_center\":\n",
        "            features = x_full\n",
        "            selected_indices = []\n",
        "            min_distances = torch.full((n_samples,), float('inf'), device=device, dtype=features.dtype)\n",
        "            current_idx = torch.randint(0, n_samples, (1,), device=device).item()\n",
        "            selected_indices.append(current_idx)\n",
        "            min_distances[current_idx] = -1.0\n",
        "\n",
        "            for i in range(1, self.coreset_size):\n",
        "                last_center_features = features[current_idx].unsqueeze(0)\n",
        "                dist_sq = torch.sum((features - last_center_features)**2, dim=1)\n",
        "                min_distances = torch.minimum(min_distances, dist_sq)\n",
        "                current_idx = torch.argmax(min_distances).item()\n",
        "                selected_indices.append(current_idx)\n",
        "                min_distances[current_idx] = -1.0\n",
        "            core_idx = torch.tensor(selected_indices, dtype=torch.long, device=device)\n",
        "            non_core_mask = torch.ones(n_samples, dtype=torch.bool, device=device)\n",
        "            non_core_mask[core_idx] = False\n",
        "            core_x, core_y = x_full[core_idx], y_full[core_idx]\n",
        "            non_core_x, non_core_y = x_full[non_core_mask], y_full[non_core_mask]\n",
        "            return core_x, core_y, non_core_x, non_core_y\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, dset, t, model_to_eval):\n",
        "        model_to_eval.eval()\n",
        "        accs = []\n",
        "        num_test_tasks = t + 1\n",
        "\n",
        "        for task_idx in range(num_test_tasks):\n",
        "            test_ds_tuple = dset.get_task(task_idx)\n",
        "            test_ds = test_ds_tuple[1]\n",
        "\n",
        "            if len(test_ds) == 0:\n",
        "                accs.append(float('nan'))\n",
        "                continue\n",
        "\n",
        "            xt, yt = test_ds.tensors[0].to(device), test_ds.tensors[1].to(device)\n",
        "            loader_test = DataLoader(TensorDataset(xt, yt), batch_size=1024)\n",
        "            task_correct, task_total = 0, 0\n",
        "            head_id_eval = 0 if self.single_head else task_idx\n",
        "\n",
        "            for xb_test, yb_test in loader_test:\n",
        "                logits = model_to_eval(xb_test, head_id_eval, sample=False)\n",
        "                preds = logits.argmax(1)\n",
        "                targets = yb_test.argmax(1) if yb_test.ndim > 1 else yb_test\n",
        "                task_correct += (preds == targets).sum().item()\n",
        "                task_total += xb_test.size(0)\n",
        "\n",
        "            accs.append(task_correct / task_total)\n",
        "\n",
        "        acc_str = \", \".join([f\"T{i}={acc}\" for i, acc in enumerate(accs)])\n",
        "        avg_acc = sum(accs) / len(accs) if accs else -233213.0\n",
        "        print(f\"After Task {t} Eval: Avg={avg_acc} | [{acc_str}]\")\n",
        "        self.acc_hist.append(accs)\n",
        "\n",
        "    def fit(self, dset, epochs=120, batch_size=None):\n",
        "        # pre-training here\n",
        "\n",
        "        opt = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "        for t in range(dset.num_tasks):\n",
        "            task_start_time = time.time()\n",
        "\n",
        "            if not self.single_head and t > 0:\n",
        "                current_task_out_dim = dset.get_task(t)[0].tensors[1].shape[1]\n",
        "                self.model.add_head(out_dim=current_task_out_dim)\n",
        "                print(f\"New head {t} for task {t} it's multihead\")\n",
        "                opt = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "\n",
        "            train_ds_t = dset.get_task(t)[0]\n",
        "            if len(train_ds_t) == 0:\n",
        "                print(f\"empty training set in task{t}\")\n",
        "                self.evaluate(dset, t, self.model)\n",
        "                continue\n",
        "\n",
        "            x_task_full, y_task_full = train_ds_t.tensors[0].to(device), train_ds_t.tensors[1].to(device)\n",
        "\n",
        "\n",
        "            core_x_t, core_y_t, x_task_non_core, y_task_non_core = self.coreset_selection(x_task_full, y_task_full)\n",
        "\n",
        "            if core_x_t is not None:\n",
        "                self.core_x.append(core_x_t.cpu())\n",
        "                self.core_y.append(core_y_t.cpu())\n",
        "\n",
        "            # just non coreset points\n",
        "            if x_task_non_core is not None and x_task_non_core.size(0) > 0:\n",
        "                current_batch_size_prop = min(batch_size, x_task_non_core.size(0))\n",
        "                loader_non_core = DataLoader(TensorDataset(x_task_non_core, y_task_non_core), batch_size=current_batch_size_prop)\n",
        "                num_train_samples_non_core = x_task_non_core.size(0)\n",
        "                self.model.train()\n",
        "                for epoch in tqdm(range(epochs), desc=f\"Task {t} learning\", leave=False):\n",
        "                    epoch_loss_prop = 0.0\n",
        "                    for xb, yb in loader_non_core:\n",
        "                        opt.zero_grad()\n",
        "                        nll = 0.0\n",
        "                        head_id = 0 if self.single_head else t\n",
        "\n",
        "                        for _ in range(self.mc):\n",
        "                            logits = self.model(xb, head_id, sample=True)\n",
        "                            nll += F.cross_entropy(logits, yb.argmax(1) if yb.ndim > 1 else yb, reduction='sum')\n",
        "\n",
        "                        kl = self.model.kl()\n",
        "                        loss = (nll / xb.size(0) / self.mc) + (kl / num_train_samples_non_core)\n",
        "                        epoch_loss_prop += loss.item() * xb.size(0)\n",
        "\n",
        "                        loss.backward()\n",
        "\n",
        "                        opt.step()\n",
        "                avg_epoch_loss_prop = epoch_loss_prop / num_train_samples_non_core\n",
        "                print(f\"Task {t}: learning finished. Avg Loss: {avg_epoch_loss_prop}\")\n",
        "            else:\n",
        "                print(f\"Task {t}: no noncorset points check out\")\n",
        "\n",
        "            # store so coreset doesn't override\n",
        "            state_after_propagation = copy.deepcopy(self.model.state_dict())\n",
        "\n",
        "            model_state_before_eval = state_after_propagation\n",
        "\n",
        "            if self.coreset_size > 0 and self.core_x and self.coreset_epochs > 0:\n",
        "\n",
        "                self.model.update_prior() # coresets are just new bayesian update so like in paper set prior\n",
        "                self.model.train()\n",
        "\n",
        "                for epoch in tqdm(range(self.coreset_epochs), desc=f\"Task {t} reminder\", leave=False):\n",
        "                    epoch_loss_re = 0.0\n",
        "                    total_samples_processed_in_epoch = 0\n",
        "\n",
        "                    if self.single_head:\n",
        "                        x_core_re = torch.cat([c.to(device) for c in self.core_x])\n",
        "                        y_core_re = torch.cat([c.to(device) for c in self.core_y])\n",
        "                        head_id_re = 0\n",
        "                        num_train_samples_core = x_core_re.size(0)\n",
        "\n",
        "                        if num_train_samples_core > 0:\n",
        "                            current_batch_size_re = min(batch_size, num_train_samples_core)\n",
        "                            loader_core = DataLoader(TensorDataset(x_core_re, y_core_re), batch_size=current_batch_size_re)\n",
        "                            for xb, yb in loader_core:\n",
        "                                opt.zero_grad()\n",
        "                                nll = 0.0\n",
        "                                for _ in range(self.mc):\n",
        "                                    logits = self.model(xb, head_id_re, sample=True)\n",
        "                                    nll += F.cross_entropy(logits, yb.argmax(1) if yb.ndim > 1 else yb, reduction='sum')\n",
        "                                kl = self.model.kl()\n",
        "                                N_t_scaling_factor = num_train_samples_non_core\n",
        "                                loss = (nll / xb.size(0) / self.mc) + (kl / N_t_scaling_factor)\n",
        "                                epoch_loss_re += loss.item() * xb.size(0)\n",
        "                                total_samples_processed_in_epoch += xb.size(0)\n",
        "                                loss.backward()\n",
        "                                opt.step()\n",
        "\n",
        "                    else:\n",
        "                        for task_idx_core in range(len(self.core_x)):\n",
        "                            if self.core_x[task_idx_core] is not None and self.core_x[task_idx_core].size(0) > 0:\n",
        "                                x_core_task = self.core_x[task_idx_core].to(device)\n",
        "                                y_core_task = self.core_y[task_idx_core].to(device)\n",
        "                                head_id_re = task_idx_core\n",
        "                                num_train_samples_core_task = x_core_task.size(0)\n",
        "                                current_batch_size_re = min(batch_size, num_train_samples_core_task)\n",
        "                                loader_core_task = DataLoader(TensorDataset(x_core_task, y_core_task), batch_size=current_batch_size_re)\n",
        "\n",
        "                                for xb, yb in loader_core_task:\n",
        "                                    opt.zero_grad()\n",
        "                                    nll = 0.0\n",
        "                                    for _ in range(self.mc):\n",
        "                                        logits = self.model(xb, head_id_re, sample=True)\n",
        "                                        nll += F.cross_entropy(logits, yb.argmax(1) if yb.ndim > 1 else yb, reduction='sum')\n",
        "\n",
        "                                    kl = self.model.kl()\n",
        "                                    loss = (nll / xb.size(0) / self.mc) + (kl / num_train_samples_non_core)\n",
        "                                    epoch_loss_re += loss.item() * xb.size(0)\n",
        "                                    total_samples_processed_in_epoch += xb.size(0)\n",
        "                                    loss.backward()\n",
        "                                    opt.step()\n",
        "\n",
        "                model_state_before_eval = copy.deepcopy(self.model.state_dict()) #\n",
        "            else:\n",
        "                print(f\"Task {t}: No coreset data available\")\n",
        "\n",
        "\n",
        "            eval_model = BayesianMLP(in_dim=self.in_dim, hidden=self.hidden_sizes, out_dim=self.n_classes,heads=len(self.model.heads), prior_var=self.prior_var).to(device)\n",
        "\n",
        "\n",
        "            eval_model.load_state_dict(model_state_before_eval)\n",
        "            self.evaluate(dset, t, eval_model)\n",
        "            del eval_model # delete never used again\n",
        "\n",
        "            # don't propagate coreset info\n",
        "            self.model.load_state_dict(state_after_propagation)\n",
        "\n",
        "            self.model.update_prior()\n",
        "\n",
        "            task_end_time = time.time()\n",
        "            print(f\"Task {t} Complete took: {task_end_time - task_start_time}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zKoaT6kvMj9d"
      },
      "outputs": [],
      "source": [
        "perm_trainer_paper = VCL(\n",
        "    in_dim=784, hidden=[100,100], n_classes=10,\n",
        "    single_head=True,\n",
        "    lr=1e-3, mc=3,\n",
        "    coreset_size=200,\n",
        "    coreset_epochs=0,\n",
        "    prior_var=1.0,\n",
        "    coreset_method = \"random\"\n",
        ")\n",
        "\n",
        "permuted_mnist_data = PermutedMNIST(num_tasks=10, seed = 100)\n",
        "perm_trainer_paper.fit(permuted_mnist_data, epochs=100, batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBlkCVS6Av1E"
      },
      "outputs": [],
      "source": [
        "perm_trainer_paper = VCL(\n",
        "    in_dim=784, hidden=[100,100], n_classes=10,\n",
        "    single_head=True,\n",
        "    lr=1e-3, mc=3,\n",
        "    coreset_size=200,\n",
        "    coreset_epochs=100,\n",
        "    prior_var=1.0,\n",
        "    coreset_method = \"random\"\n",
        ")\n",
        "\n",
        "permuted_mnist_data = PermutedMNIST(num_tasks=10, seed = 100)\n",
        "perm_trainer_paper.fit(permuted_mnist_data, epochs=100, batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyufjHT6ih1K"
      },
      "outputs": [],
      "source": [
        "perm_trainer_paper = VCL(\n",
        "    in_dim=784, hidden=[256,256], n_classes=2,\n",
        "    single_head=False,\n",
        "    lr=1e-3, mc=3,\n",
        "    coreset_size=200,\n",
        "    coreset_epochs=0,\n",
        "    prior_var=1.0,\n",
        "    coreset_method = \"random\"\n",
        ")\n",
        "\n",
        "split_mnist = SplitMNIST()\n",
        "perm_trainer_paper.fit(split_mnist, epochs=100, batch_size=7000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJj5ao0JHjnh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}