{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y65niXUmMQxO"
      },
      "outputs": [],
      "source": [
        "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np, random, os, time, tarfile, gc\n",
        "import copy\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device\", device)\n",
        "torch.manual_seed(1); random.seed(1); np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PermutedMNIST:\n",
        "    def __init__(self, num_tasks=10, seed=123):\n",
        "        shuffler = torch.Generator().manual_seed(seed)\n",
        "        self.perms = [torch.randperm(784, generator=shuffler) for _ in range(num_tasks)]\n",
        "        tfm = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "        tr = datasets.MNIST(\"./data\", True,  download=True, transform=tfm)\n",
        "        te = datasets.MNIST(\"./data\", False, download=True, transform=tfm)\n",
        "        x_tr = tr.data.float().view(-1,784)/255.\n",
        "        x_te = te.data.float().view(-1,784)/255.\n",
        "        y_tr = F.one_hot(tr.targets, 10).float()\n",
        "        y_te = F.one_hot(te.targets, 10).float()\n",
        "        self.tasks = [(TensorDataset(x_tr[:, p], y_tr), TensorDataset(x_te[:, p], y_te))for p in self.perms]\n",
        "        self.input_dim = 784\n",
        "        self.n_classes = 10\n",
        "        self.num_tasks = num_tasks\n",
        "    def get_task(self, tid):\n",
        "        return self.tasks[tid]\n",
        "\n",
        "class SplitMNIST:\n",
        "    pairs = [(0,1), (2,3), (4,5), (6,7), (8,9)]\n",
        "    def __init__(self):\n",
        "        tfm = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "        tr = datasets.MNIST(\"./data\", True,  download=True, transform=tfm)\n",
        "        te = datasets.MNIST(\"./data\", False, download=True, transform=tfm)\n",
        "        x_tr = tr.data.float().view(-1,784)/255.\n",
        "        x_te = te.data.float().view(-1,784)/255.\n",
        "        self.tasks=[]\n",
        "        for a, b in self.pairs:\n",
        "            msk_tr = (tr.targets==a)|(tr.targets==b)\n",
        "            msk_te = (te.targets==a)|(te.targets==b)\n",
        "            y_tr = F.one_hot((tr.targets[msk_tr]==b).long(), 2).float()\n",
        "            y_te = F.one_hot((te.targets[msk_te]==b).long(), 2).float()\n",
        "            self.tasks.append((TensorDataset(x_tr[msk_tr], y_tr), TensorDataset(x_te[msk_te], y_te)))\n",
        "        self.input_dim = 784\n",
        "        self.n_classes = 2\n",
        "        self.num_tasks = 5\n",
        "    def get_task(self, tid):\n",
        "        return self.tasks[tid]"
      ],
      "metadata": {
        "id": "8WwhsojhMTXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PlainMLP(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super().__init__()\n",
        "        layers=[]\n",
        "        for din,dout in zip(dims[:-1],dims[1:]):\n",
        "            layers.append(nn.Linear(din,dout))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.pop() # last one no relu\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "class BayesianLinear(nn.Module):\n",
        "    def __init__(self, in_f, out_f, prior_var=1.0):\n",
        "        super().__init__()\n",
        "        self.w_mu     = nn.Parameter(torch.empty(out_f,in_f))\n",
        "        self.w_logvar = nn.Parameter(torch.full((out_f,in_f), -6.0))\n",
        "        self.b_mu     = nn.Parameter(torch.empty(out_f))\n",
        "        self.b_logvar = nn.Parameter(torch.full((out_f,), -6.0))\n",
        "        nn.init.normal_(self.w_mu,0,0.1); nn.init.normal_(self.b_mu,0,0.1)\n",
        "        self.register_buffer(\"pw_mu\", torch.zeros_like(self.w_mu))\n",
        "        self.register_buffer(\"pw_logvar\", torch.full_like(self.w_mu, math.log(prior_var)))\n",
        "        self.register_buffer(\"pb_mu\", torch.zeros_like(self.b_mu))\n",
        "        self.register_buffer(\"pb_logvar\", torch.full_like(self.b_mu, math.log(prior_var)))\n",
        "    def _sample(self):\n",
        "        ew = torch.randn_like(self.w_mu)\n",
        "        eb = torch.randn_like(self.b_mu)\n",
        "        w = self.w_mu + (0.5*self.w_logvar).exp()*ew\n",
        "        b = self.b_mu + (0.5*self.b_logvar).exp()*eb\n",
        "        return w,b\n",
        "    def forward(self,x,sample=True):\n",
        "        w,b = self._sample() if sample else (self.w_mu, self.b_mu)\n",
        "        return F.linear(x,w,b)\n",
        "    def helper_kl(self, m, lv, m0, lv0):\n",
        "        v, v0 = lv.exp(), lv0.exp()\n",
        "        return 0.5*((lv0-lv) + (v+(m-m0).pow(2))/v0 -1).sum()\n",
        "    def kl(self):\n",
        "        return self.helper_kl(self.w_mu,self.w_logvar,self.pw_mu,self.pw_logvar) + self.helper_kl(self.b_mu,self.b_logvar,self.pb_mu,self.pb_logvar)\n",
        "    def update_prior(self):\n",
        "        self.pw_mu.data.copy_(self.w_mu.data)\n",
        "        self.pw_logvar.data.copy_(self.w_logvar.data)\n",
        "        self.pb_mu.data.copy_(self.b_mu.data)\n",
        "        self.pb_logvar.data.copy_(self.b_logvar.data)\n",
        "\n",
        "class BayesianMLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, out_dim, heads=1, prior_var=1.0):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.ModuleList()\n",
        "        last = in_dim\n",
        "        for h in hidden:\n",
        "            self.hidden.append(BayesianLinear(last,h,prior_var))\n",
        "            last = h\n",
        "        self.heads = nn.ModuleList([BayesianLinear(last,out_dim,prior_var) for _ in range(heads)])\n",
        "        self.out_dim = out_dim\n",
        "    def add_head(self, out_dim):\n",
        "        head = BayesianLinear(self.hidden[-1].w_mu.size(0), out_dim)\n",
        "        head.to(next(self.parameters()).device); self.heads.append(head)\n",
        "    def forward(self,x,head_id=0,sample=True):\n",
        "        for l in self.hidden: x = torch.relu(l(x,sample))\n",
        "        return self.heads[head_id](x,sample)\n",
        "    def kl(self):\n",
        "        return sum(l.kl() for l in self.hidden)+sum(h.kl() for h in self.heads)\n",
        "    def update_prior(self):\n",
        "        for l in self.hidden: l.update_prior()\n",
        "        for h in self.heads:  h.update_prior()\n"
      ],
      "metadata": {
        "id": "DVQuXfM8MbE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VCLGlobalSigma:\n",
        "    def __init__(self, in_dim, hidden, n_classes,\n",
        "                 single_head=True, lr=1e-4, mc=3,\n",
        "                 init_log_sigma=-3.0,\n",
        "                 beta=1, prior_var=1.0,\n",
        "                 coreset_size=0, coreset_epochs=0):\n",
        "\n",
        "        self.in_dim, self.hidden_sizes = in_dim, list(hidden)\n",
        "        self.n_classes = n_classes\n",
        "        self.single_head = single_head\n",
        "        self.beta            = beta\n",
        "        self.lr, self.mc = lr, mc\n",
        "        self.prior_var = prior_var\n",
        "        self.print_freq      = 5 # debug change\n",
        "        self.coreset_size    = coreset_size\n",
        "        self.coreset_epochs = coreset_epochs\n",
        "\n",
        "        self.model = BayesianMLP(in_dim=self.in_dim, hidden=self.hidden_sizes, out_dim=self.n_classes, heads=1, prior_var=self.prior_var).to(device)\n",
        "\n",
        "        self.log_sigma = nn.Parameter(torch.tensor(float(init_log_sigma)))\n",
        "        self.model.register_parameter(\"log_sigma\", self.log_sigma)\n",
        "\n",
        "        self.rmse_hist = []\n",
        "        self.core_x, self.core_y = [], []\n",
        "\n",
        "        init_sig2 = torch.exp(2 * self.log_sigma).item()\n",
        "        print(f\"sigma init={init_sig2} \" + f\"beta = {self.beta}, lr={self.lr}, coreset size of {self.coreset_size}\")\n",
        "\n",
        "    # for LR just do random other method is pretty much identical in performance to permuted mnist\n",
        "    def coreset_selection(self, x_full, y_full):\n",
        "        n_samples = x_full.size(0)\n",
        "        if self.coreset_size <= 0:\n",
        "            return None, None, x_full, y_full # don't do coresets.\n",
        "        if self.coreset_size >= n_samples:\n",
        "             print(\"Coreset size > train size\")\n",
        "             return x_full, y_full, None, None\n",
        "\n",
        "        perm = torch.randperm(n_samples, device=x_full.device)\n",
        "        core_idx = perm[:self.coreset_size]\n",
        "        non_core_mask = torch.ones(n_samples, dtype=torch.bool, device=x_full.device)\n",
        "        non_core_mask[core_idx] = False\n",
        "\n",
        "        core_x, core_y = x_full[core_idx], y_full[core_idx]\n",
        "        non_core_x, non_core_y = x_full[non_core_mask], y_full[non_core_mask]\n",
        "\n",
        "        return core_x, core_y, non_core_x, non_core_y\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, dset, t, model_to_eval):\n",
        "        model_to_eval.eval()\n",
        "        rmses = []\n",
        "        overall_sq_sum = 0.0\n",
        "        overall_n = 0\n",
        "        num_eval_tasks = t + 1\n",
        "        for task_idx in range(num_eval_tasks):\n",
        "            _, test_ds = dset.get_task(task_idx)\n",
        "            if len(test_ds) == 0:\n",
        "                 rmses.append(float('nan'))\n",
        "                 continue\n",
        "            loader = DataLoader(test_ds, batch_size=1024, shuffle=False)\n",
        "            sq_sum, n_elem = 0.0, 0\n",
        "            head_id_eval = 0\n",
        "            for xb_test, yb_test in loader:\n",
        "                xb_test, yb_test = xb_test.to(device), yb_test.to(device)\n",
        "                pred = model_to_eval(xb_test, head_id=head_id_eval, sample=False)\n",
        "                sq_sum += (pred - yb_test).pow(2).sum().item()\n",
        "                n_elem += yb_test.numel()\n",
        "                overall_sq_sum += (pred - yb_test).pow(2).sum().item()\n",
        "                overall_n += yb_test.numel()\n",
        "            rmse = math.sqrt(sq_sum / n_elem) if n_elem > 0 else 0.0\n",
        "            rmses.append(rmse)\n",
        "        valid_rmses = [r for r in rmses if not math.isnan(r)]\n",
        "        overall = math.sqrt(overall_sq_sum/overall_n) if valid_rmses else -1223334444\n",
        "        self.rmse_hist.append((rmses, overall))\n",
        "        tasks_str = \", \".join(f\"T{i}={r}\" for i, r in enumerate(rmses))\n",
        "        print(f\"After Task {t}: full RMSE={overall} | [{tasks_str}]\")\n",
        "\n",
        "    def fit(self, dset, epochs=50, batch_size=256):\n",
        "        start_fit_time = time.time()\n",
        "        head_id = 0\n",
        "\n",
        "        opt = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "        for t in range(dset.num_tasks):\n",
        "            task_start_time = time.time()\n",
        "            tr_ds, _ = dset.get_task(t)\n",
        "\n",
        "            x, y = tr_ds.tensors[0].to(device), tr_ds.tensors[1].to(device)\n",
        "\n",
        "            cx, cy, x_nc, y_nc = self.coreset_selection(x, y)\n",
        "            if cx is not None:\n",
        "                self.core_x.append(cx.cpu())\n",
        "                self.core_y.append(cy.cpu())\n",
        "\n",
        "            if x_nc is not None and x_nc.size(0) > 0:\n",
        "                num_train_samples_nc = x_nc.size(0)\n",
        "                current_batch_size_nc = min(batch_size, num_train_samples_nc)\n",
        "                loader_nc = DataLoader(TensorDataset(x_nc, y_nc), batch_size=current_batch_size_nc, shuffle=True)\n",
        "\n",
        "                for ep in tqdm(range(epochs), desc=f\"Task {t} learning\", leave=False):\n",
        "                    self.model.train()\n",
        "                    for batch_idx, (xb, yb) in enumerate(loader_nc):\n",
        "                        xb, yb = xb.to(device), yb.to(device)\n",
        "                        opt.zero_grad()\n",
        "\n",
        "                        batch_nll = 0.0; batch_mse = 0.0\n",
        "                        inv_var = torch.exp(-2 * self.log_sigma)\n",
        "                        out_dim = yb.size(1) if yb.ndim > 1 else 1\n",
        "                        log_2pi = math.log(2 * math.pi)\n",
        "\n",
        "                        for _ in range(self.mc):\n",
        "                            out = self.model(xb, head_id=head_id, sample=True)\n",
        "                            se_sum = (out - yb).pow(2).sum()\n",
        "                            nll_mc_sample = (0.5 * xb.size(0) * out_dim * log_2pi) + (xb.size(0) * out_dim * self.log_sigma) + (0.5 * inv_var * se_sum)\n",
        "                            batch_nll += nll_mc_sample\n",
        "                            batch_mse += se_sum.item()\n",
        "                        nll = batch_nll / self.mc\n",
        "                        mse = batch_mse / (self.mc * xb.size(0) * out_dim)\n",
        "                        kl = self.model.kl()\n",
        "\n",
        "                        loss = nll / xb.size(0) + self.beta * kl / xb.size(0)\n",
        "\n",
        "                        loss.backward()\n",
        "                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                        opt.step()\n",
        "\n",
        "                        # debug\n",
        "                        if (batch_idx == 0 and ep % self.print_freq == 0):\n",
        "                            nll_s = nll.item() / xb.size(0)\n",
        "                            kl_s = kl.item() / xb.size(0)\n",
        "                            ratio = (self.beta * kl_s) / nll_s\n",
        "                            sig2 = torch.exp(2 * self.log_sigma).item()\n",
        "                            print(f\"Task {t} Ep {ep} \" + f\"L={loss.item()} NLL={nll_s} + b*KL={self.beta * kl_s}) \"\n",
        "                                  + f\" KL/NLL={ratio}  RMSE={math.sqrt(mse)}  var={sig2}\")\n",
        "                print(f\"Task {t} non coreset done\")\n",
        "            else:\n",
        "                print(f\"Task {t}: error 123\")\n",
        "\n",
        "            # store so coreset doesn't override\n",
        "            state_after_propagation = copy.deepcopy(self.model.state_dict())\n",
        "\n",
        "            model_state_before_eval = state_after_propagation\n",
        "\n",
        "            if self.coreset_epochs > 0 and self.core_x:\n",
        "                cx_all = torch.cat(self.core_x).to(device); cy_all = torch.cat(self.core_y).to(device)\n",
        "                num_train_samples_core = cx_all.size(0)\n",
        "                if num_train_samples_core > 0:\n",
        "                    current_batch_size_core = min(batch_size, num_train_samples_core)\n",
        "                    loader_core = DataLoader(TensorDataset(cx_all, cy_all), batch_size=current_batch_size_core, shuffle=True)\n",
        "\n",
        "                    self.model.update_prior()\n",
        "\n",
        "                    for ep in tqdm(range(self.coreset_epochs), desc=f\"Task {t} rem\", leave=False):\n",
        "                        self.model.train()\n",
        "                        for batch_idx, (xb, yb) in enumerate(loader_core):\n",
        "                            xb, yb = xb.to(device), yb.to(device)\n",
        "                            opt.zero_grad()\n",
        "\n",
        "                            batch_nll = 0.0; batch_mse = 0.0\n",
        "                            inv_var = torch.exp(-2 * self.log_sigma)\n",
        "                            out_dim = yb.size(1) if yb.ndim > 1 else 1\n",
        "                            log_2pi = math.log(2 * math.pi)\n",
        "\n",
        "                            for _ in range(self.mc):\n",
        "                                out = self.model(xb, head_id=head_id, sample=True)\n",
        "                                se_sum = (out - yb).pow(2).sum()\n",
        "                                nll_mc_sample = (0.5 * xb.size(0) * out_dim * log_2pi) + (xb.size(0) * out_dim * self.log_sigma) + (0.5 * inv_var * se_sum)\n",
        "                                batch_nll += nll_mc_sample\n",
        "                                batch_mse += se_sum.item()\n",
        "\n",
        "                            nll = batch_nll / self.mc\n",
        "                            mse = batch_mse / (self.mc * xb.size(0) * out_dim)\n",
        "                            kl = self.model.kl()\n",
        "                            loss = nll / xb.size(0) + self.beta * kl / xb.size(0)\n",
        "\n",
        "                            loss.backward()\n",
        "                            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
        "                            opt.step()\n",
        "\n",
        "                    print(f\"Task {t} coreset done.\")\n",
        "                    model_state_before_eval = copy.deepcopy(self.model.state_dict())\n",
        "                else:\n",
        "                     print(f\"Task {t} no coreset error tr43\")\n",
        "            else:\n",
        "                print(f\"Task {t} no coreset\")\n",
        "\n",
        "            eval_model = BayesianMLP(in_dim=self.in_dim, hidden=self.hidden_sizes, out_dim=self.n_classes, heads=1, prior_var=self.prior_var).to(device)\n",
        "            eval_model.register_parameter(\"log_sigma\", self.log_sigma)\n",
        "            eval_model.load_state_dict(model_state_before_eval)\n",
        "            self.evaluate(dset, t, eval_model)\n",
        "\n",
        "            self.model.load_state_dict(state_after_propagation)\n",
        "\n",
        "            del eval_model\n",
        "\n",
        "            self.model.update_prior()\n",
        "\n",
        "            task_end_time = time.time()\n",
        "\n",
        "        end_fit_time = time.time()\n",
        "        print(f\"took {end_fit_time - start_fit_time} s\")"
      ],
      "metadata": {
        "id": "yk5_aSoSsFSP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = VCLGlobalSigma(in_dim=784, hidden=[100, 100],\n",
        "n_classes=10, single_head=True, init_log_sigma=-1.5, beta=1.0, lr=1e-4, mc=3,\n",
        "prior_var=1.0, coreset_size=200, coreset_epochs=0)\n",
        "\n",
        "permuted_mnist_data = PermutedMNIST(num_tasks=10)\n",
        "trainer.fit(permuted_mnist_data, epochs=50, batch_size=256)"
      ],
      "metadata": {
        "id": "4nGkXe4Ksdb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KhRJnLYska1x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}