{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y65niXUmMQxO"
      },
      "outputs": [],
      "source": [
        "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np, random, os, time, tarfile, gc, heapq\n",
        "import copy\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device\", device)\n",
        "torch.manual_seed(1); random.seed(1); np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WwhsojhMTXP"
      },
      "outputs": [],
      "source": [
        "class PermutedMNIST:\n",
        "    def __init__(self, num_tasks=10, seed=123):\n",
        "        shuffler = torch.Generator().manual_seed(seed)\n",
        "        self.perms = [torch.randperm(784, generator=shuffler) for _ in range(num_tasks)]\n",
        "        tfm = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "        tr = datasets.MNIST(\"./data\", True,  download=True, transform=tfm)\n",
        "        te = datasets.MNIST(\"./data\", False, download=True, transform=tfm)\n",
        "        x_tr = tr.data.float().view(-1,784)/255.\n",
        "        x_te = te.data.float().view(-1,784)/255.\n",
        "        y_tr = F.one_hot(tr.targets, 10).float()\n",
        "        y_te = F.one_hot(te.targets, 10).float()\n",
        "        self.tasks = [(TensorDataset(x_tr[:, p], y_tr), TensorDataset(x_te[:, p], y_te))for p in self.perms]\n",
        "        self.input_dim = 784\n",
        "        self.n_classes = 10\n",
        "        self.num_tasks = num_tasks\n",
        "    def get_task(self, tid):\n",
        "        return self.tasks[tid]\n",
        "\n",
        "class SplitMNIST:\n",
        "    pairs = [(0,1), (2,3), (4,5), (6,7), (8,9)]\n",
        "    def __init__(self):\n",
        "        tfm = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "        tr = datasets.MNIST(\"./data\", True,  download=True, transform=tfm)\n",
        "        te = datasets.MNIST(\"./data\", False, download=True, transform=tfm)\n",
        "        x_tr = tr.data.float().view(-1,784)/255.\n",
        "        x_te = te.data.float().view(-1,784)/255.\n",
        "        self.tasks=[]\n",
        "        for a, b in self.pairs:\n",
        "            msk_tr = (tr.targets==a)|(tr.targets==b)\n",
        "            msk_te = (te.targets==a)|(te.targets==b)\n",
        "            y_tr = F.one_hot((tr.targets[msk_tr]==b).long(), 2).float()\n",
        "            y_te = F.one_hot((te.targets[msk_te]==b).long(), 2).float()\n",
        "            self.tasks.append((TensorDataset(x_tr[msk_tr], y_tr), TensorDataset(x_te[msk_te], y_te)))\n",
        "        self.input_dim = 784\n",
        "        self.n_classes = 2\n",
        "        self.num_tasks = 5\n",
        "    def get_task(self, tid):\n",
        "        return self.tasks[tid]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkvtRvFOMdm8"
      },
      "outputs": [],
      "source": [
        "class PlainMLP(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super().__init__()\n",
        "        layers=[]\n",
        "        for din,dout in zip(dims[:-1],dims[1:]):\n",
        "            layers.append(nn.Linear(din,dout))\n",
        "            layers.append(nn.ReLU())\n",
        "        layers.pop() # last one no relu\n",
        "        self.net = nn.Sequential(*layers)\n",
        "    def forward(self,x): return self.net(x)\n",
        "\n",
        "class BayesianLinear(nn.Module):\n",
        "    def __init__(self, in_f, out_f, prior_var=1.0):\n",
        "        super().__init__()\n",
        "        self.w_mu     = nn.Parameter(torch.empty(out_f,in_f))\n",
        "        self.w_logvar = nn.Parameter(torch.full((out_f,in_f), -6.0))\n",
        "        self.b_mu     = nn.Parameter(torch.empty(out_f))\n",
        "        self.b_logvar = nn.Parameter(torch.full((out_f,), -6.0))\n",
        "        nn.init.normal_(self.w_mu,0,0.1); nn.init.normal_(self.b_mu,0,0.1)\n",
        "        self.register_buffer(\"pw_mu\", torch.zeros_like(self.w_mu))\n",
        "        self.register_buffer(\"pw_logvar\", torch.full_like(self.w_mu, math.log(prior_var)))\n",
        "        self.register_buffer(\"pb_mu\", torch.zeros_like(self.b_mu))\n",
        "        self.register_buffer(\"pb_logvar\", torch.full_like(self.b_mu, math.log(prior_var)))\n",
        "    def sample(self):\n",
        "        ew = torch.randn_like(self.w_mu)\n",
        "        eb = torch.randn_like(self.b_mu)\n",
        "        w = self.w_mu + (0.5*self.w_logvar).exp()*ew\n",
        "        b = self.b_mu + (0.5*self.b_logvar).exp()*eb\n",
        "        return w,b\n",
        "    def forward(self,x,sample=True):\n",
        "        w,b = self.sample() if sample else (self.w_mu, self.b_mu)\n",
        "        return F.linear(x,w,b)\n",
        "    def helper_kl(self, m, lv, m0, lv0):\n",
        "        v, v0 = lv.exp(), lv0.exp()\n",
        "        return 0.5*((lv0-lv) + (v+(m-m0).pow(2))/v0 -1).sum()\n",
        "    def kl(self):\n",
        "        return self.helper_kl(self.w_mu,self.w_logvar,self.pw_mu,self.pw_logvar) + self.helper_kl(self.b_mu,self.b_logvar,self.pb_mu,self.pb_logvar)\n",
        "    def update_prior(self):\n",
        "        self.pw_mu.data.copy_(self.w_mu.data)\n",
        "        self.pw_logvar.data.copy_(self.w_logvar.data)\n",
        "        self.pb_mu.data.copy_(self.b_mu.data)\n",
        "        self.pb_logvar.data.copy_(self.b_logvar.data)\n",
        "\n",
        "class BayesianMLP(nn.Module):\n",
        "    def __init__(self, in_dim, hidden, out_dim, heads=1, prior_var=1.0):\n",
        "        super().__init__()\n",
        "        self.hidden = nn.ModuleList()\n",
        "        last = in_dim\n",
        "        for h in hidden:\n",
        "            self.hidden.append(BayesianLinear(last,h,prior_var))\n",
        "            last = h\n",
        "        self.heads = nn.ModuleList([BayesianLinear(last,out_dim,prior_var) for _ in range(heads)])\n",
        "        self.out_dim = out_dim\n",
        "    def add_head(self, out_dim):\n",
        "        head = BayesianLinear(self.hidden[-1].w_mu.size(0), out_dim)\n",
        "        head.to(next(self.parameters()).device); self.heads.append(head)\n",
        "    def forward(self,x,head_id=0,sample=True):\n",
        "        for l in self.hidden: x = torch.relu(l(x,sample))\n",
        "        return self.heads[head_id](x,sample)\n",
        "    def kl(self):\n",
        "        return sum(l.kl() for l in self.hidden)+sum(h.kl() for h in self.heads)\n",
        "    def update_prior(self):\n",
        "        for l in self.hidden: l.update_prior()\n",
        "        for h in self.heads:  h.update_prior()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMJtsjJiMh0R"
      },
      "outputs": [],
      "source": [
        "class VCLReservoir:\n",
        "    def __init__(self, in_dim, hidden, n_classes,\n",
        "                 single_head=True, lr=1e-3, mc=10,\n",
        "                 prior_var=1.0, coreset_size=100,\n",
        "                 coreset_epochs=20,\n",
        "                 weighting_scheme=\"uniform\"):\n",
        "\n",
        "\n",
        "        self.in_dim, self.hidden_sizes = in_dim, list(hidden)\n",
        "        self.n_classes = n_classes\n",
        "        self.single_head = single_head\n",
        "        self.lr, self.mc = lr, mc\n",
        "        self.prior_var = prior_var\n",
        "        self.coreset_size = coreset_size\n",
        "        self.coreset_epochs = coreset_epochs\n",
        "        self.weighting_scheme = weighting_scheme\n",
        "\n",
        "        # reservoir\n",
        "        feature_dim = in_dim\n",
        "        self.reservoir_x = torch.zeros((coreset_size, feature_dim), device=device, dtype=torch.float)\n",
        "        self.reservoir_y = torch.zeros(coreset_size, device=device, dtype=torch.long)\n",
        "        self.reservoir_task_ids = torch.zeros(coreset_size, device=device, dtype=torch.long)\n",
        "        self.reservoir_heap = []\n",
        "        self.current_fill = 0\n",
        "        self.min_key_in_heap = float('inf')\n",
        "\n",
        "        init_heads = 1\n",
        "        initial_out_dim = n_classes\n",
        "        self.model = BayesianMLP(in_dim=self.in_dim, hidden=self.hidden_sizes, out_dim=initial_out_dim, heads=init_heads, prior_var=self.prior_var).to(device)\n",
        "\n",
        "        self.acc_hist = []\n",
        "\n",
        "        print(f\"Coreset Size {self.coreset_size}, w_function '{self.weighting_scheme}', coreset_epochs {self.coreset_epochs}\")\n",
        "\n",
        "    def calculate_weights(self, task_id, num_samples):\n",
        "        if self.weighting_scheme == \"uniform\":\n",
        "            return torch.ones(num_samples, device=device)\n",
        "        elif self.weighting_scheme == \"task_power_2\": #debug\n",
        "            weight_val = 2.0**task_id\n",
        "            return torch.full((num_samples,), fill_value=weight_val, device=device)\n",
        "        elif self.weighting_scheme == \"geom_09\":\n",
        "            weight_val = 0.9**task_id\n",
        "            return torch.full((num_samples,), fill_value=weight_val, device=device)\n",
        "        else:\n",
        "            print(f\"not implemented yet\")\n",
        "            return torch.ones(num_samples, device=device)\n",
        "\n",
        "    # give on device\n",
        "    def update_reservoir(self, x_batch_gpu, y_batch_gpu, task_id):\n",
        "        n_batch = x_batch_gpu.size(0)\n",
        "        if n_batch == 0: return\n",
        "\n",
        "        weights = self.calculate_weights(task_id, n_batch)\n",
        "\n",
        "        if y_batch_gpu.ndim > 1 and y_batch_gpu.shape[1] > 1:\n",
        "             y_batch_gpu = y_batch_gpu.argmax(dim=1)\n",
        "\n",
        "        for i in range(n_batch):\n",
        "            w_i = weights[i].item()\n",
        "            rand_i = torch.rand(1).item()\n",
        "            if rand_i < 1e-9: rand_i = 1e-9\n",
        "            key_i = rand_i ** (1.0 / w_i)\n",
        "\n",
        "            if self.current_fill < self.coreset_size:\n",
        "                slot_idx = self.current_fill\n",
        "                self.reservoir_x[slot_idx] = x_batch_gpu[i]\n",
        "                self.reservoir_y[slot_idx] = y_batch_gpu[i]\n",
        "                self.reservoir_task_ids[slot_idx] = task_id\n",
        "                heapq.heappush(self.reservoir_heap, (key_i, slot_idx))\n",
        "                self.current_fill += 1\n",
        "                self.min_key_in_heap = self.reservoir_heap[0][0]\n",
        "\n",
        "            elif key_i > self.min_key_in_heap:\n",
        "                popped_key, slot_to_replace = heapq.heappop(self.reservoir_heap)\n",
        "                heapq.heappush(self.reservoir_heap, (key_i, slot_to_replace))\n",
        "                self.reservoir_x[slot_to_replace] = x_batch_gpu[i]\n",
        "                self.reservoir_y[slot_to_replace] = y_batch_gpu[i]\n",
        "                self.reservoir_task_ids[slot_to_replace] = task_id\n",
        "                self.min_key_in_heap = self.reservoir_heap[0][0]\n",
        "\n",
        "    # debug\n",
        "    def print_reservoir_proportions(self):\n",
        "        valid_task_ids = self.reservoir_task_ids[:self.current_fill]\n",
        "        unique_tasks, counts = torch.unique(valid_task_ids, sorted=True, return_counts=True)\n",
        "        props = []\n",
        "        total_selected = self.current_fill\n",
        "        for tid, count in zip(unique_tasks.tolist(), counts.tolist()):\n",
        "            prop = count / total_selected * 100\n",
        "            props.append(f\"T{tid}={prop}% ({count})\")\n",
        "\n",
        "        print(f\"Reservoir Composition {total_selected}/{self.coreset_size} points: {', '.join(props)}\")\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, dset, t, model_to_eval):\n",
        "        model_to_eval.eval() # eval time\n",
        "        accs = []\n",
        "        num_test_tasks = t + 1\n",
        "\n",
        "        for task_idx in range(num_test_tasks):\n",
        "            try:\n",
        "                test_ds_tuple = dset.get_task(task_idx)\n",
        "                test_ds = test_ds_tuple[1]\n",
        "\n",
        "                xt, yt = test_ds.tensors[0].to(device), test_ds.tensors[1].to(device)\n",
        "                loader_test = DataLoader(TensorDataset(xt, yt), batch_size=1024) # can adjust\n",
        "                task_correct, task_total = 0, 0\n",
        "                head_id_eval = 0 if self.single_head else task_idx\n",
        "\n",
        "                for xb_test, yb_test in loader_test:\n",
        "                    logits = model_to_eval(xb_test, head_id_eval, sample=False)\n",
        "                    preds = logits.argmax(1)\n",
        "                    targets = yb_test.argmax(1) if yb_test.ndim > 1 and yb_test.shape[1] > 1 else yb_test # Handle one-hot/scalar labels\n",
        "                    task_correct += (preds == targets).sum().item()\n",
        "                    task_total += xb_test.size(0)\n",
        "\n",
        "                accs.append(task_correct / task_total)\n",
        "            except Exception as e:\n",
        "                 print(f\"Error 111 task {task_idx}: {e}\")\n",
        "                 accs.append(float('nan'))\n",
        "\n",
        "        acc_str = \", \".join([f\"T{i}={acc}\" for i, acc in enumerate(accs)])\n",
        "        avg_acc = sum(accs) / len(accs) if accs else -233213.0\n",
        "        print(f\"After Task {t} Eval: Avg={avg_acc} | [{acc_str}]\")\n",
        "        self.acc_hist.append(accs)\n",
        "\n",
        "\n",
        "    def fit(self, dset, epochs=120, batch_size=None):\n",
        "        opt = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "        for t in range(dset.num_tasks):\n",
        "            task_start_time = time.time()\n",
        "\n",
        "            if not self.single_head and t > 0:\n",
        "                current_task_out_dim = dset.get_task(t)[0].tensors[1].shape[1]\n",
        "                self.model.add_head(out_dim=current_task_out_dim)\n",
        "                print(f\"New head {t} for task {t} it's multihead\")\n",
        "                opt = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n",
        "\n",
        "\n",
        "\n",
        "            train_ds_t_tuple = dset.get_task(t)\n",
        "            train_ds_t = train_ds_t_tuple[0]\n",
        "\n",
        "            x_task_full, y_task_full = train_ds_t.tensors[0].to(device), train_ds_t.tensors[1].to(device)\n",
        "\n",
        "            if self.reservoir_y.dtype == torch.long and y_task_full.ndim > 1 and y_task_full.shape[1] > 1:\n",
        "                y_task_full = y_task_full.argmax(dim=1)\n",
        "\n",
        "            self.update_reservoir(x_task_full, y_task_full, t)\n",
        "            self.print_reservoir_proportions()\n",
        "\n",
        "\n",
        "\n",
        "            if x_task_full.size(0) > 0:\n",
        "                current_batch_size_prop = min(batch_size, x_task_full.size(0))\n",
        "                loader_prop = DataLoader(TensorDataset(x_task_full, y_task_full), batch_size=current_batch_size_prop, shuffle=True)\n",
        "                num_train_samples = x_task_full.size(0)\n",
        "\n",
        "                self.model.train()\n",
        "                for epoch in tqdm(range(epochs), desc=f\"Task {t} main loop\", leave=False):\n",
        "                    epoch_loss_prop = 0.0\n",
        "                    for xb, yb in loader_prop:\n",
        "                         opt.zero_grad()\n",
        "                         nll = 0.0\n",
        "                         head_id = 0 if self.single_head else t\n",
        "                         for _ in range(self.mc):\n",
        "                              logits = self.model(xb, head_id, sample=True)\n",
        "                              targets = yb.argmax(1) if yb.ndim > 1 and yb.shape[1] > 1 else yb\n",
        "                              nll += F.cross_entropy(logits, targets, reduction='sum')\n",
        "                         kl = self.model.kl()\n",
        "                         loss = (nll / xb.size(0) / self.mc) + (kl / num_train_samples)\n",
        "                         epoch_loss_prop += loss.item() * xb.size(0)\n",
        "                         loss.backward()\n",
        "                         opt.step()\n",
        "            else:\n",
        "                print(f\"Task {t}: no noncorset points check out\")\n",
        "\n",
        "\n",
        "            state_after_propagation = copy.deepcopy(self.model.state_dict())\n",
        "            model_state_before_eval = state_after_propagation\n",
        "\n",
        "            if self.coreset_size > 0 and self.current_fill > 0 and self.coreset_epochs > 0:\n",
        "                x_core_re = self.reservoir_x[:self.current_fill]\n",
        "                y_core_re = self.reservoir_y[:self.current_fill]\n",
        "                head_id_re = 0\n",
        "\n",
        "                self.model.update_prior()\n",
        "\n",
        "                num_train_samples_coreset = x_core_re.size(0)\n",
        "                current_batch_size_re = min(batch_size, num_train_samples_coreset)\n",
        "                loader_core = DataLoader(TensorDataset(x_core_re, y_core_re), batch_size=current_batch_size_re, shuffle=True)\n",
        "\n",
        "                self.model.train()\n",
        "\n",
        "                for epoch in tqdm(range(self.coreset_epochs), desc=f\"Coreset {t} loop\", leave=False):\n",
        "                      epoch_loss_re = 0.0\n",
        "                      for xb, yb in loader_core:\n",
        "                          opt.zero_grad()\n",
        "                          nll = 0.0\n",
        "\n",
        "                          for _ in range(self.mc):\n",
        "                                logits = self.model(xb, head_id_re, sample=True)\n",
        "                                targets = yb.argmax(1) if yb.ndim > 1 and yb.shape[1] > 1 else yb\n",
        "                                nll += F.cross_entropy(logits, targets, reduction='sum')\n",
        "                          kl = self.model.kl()\n",
        "                          loss = (nll / xb.size(0) / self.mc) + (kl / num_train_samples_coreset)\n",
        "                          epoch_loss_re += loss.item() * xb.size(0)\n",
        "                          loss.backward()\n",
        "                          opt.step()\n",
        "                model_state_before_eval = copy.deepcopy(self.model.state_dict())\n",
        "            else:\n",
        "                 print(f\"Task {t} no coreset\")\n",
        "\n",
        "            eval_model = BayesianMLP(in_dim=self.in_dim, hidden=self.hidden_sizes, out_dim=self.n_classes, heads=len(self.model.heads), prior_var=self.prior_var).to(device)\n",
        "            eval_model.load_state_dict(model_state_before_eval)\n",
        "\n",
        "            self.evaluate(dset, t, eval_model)\n",
        "            del eval_model\n",
        "\n",
        "            self.model.load_state_dict(state_after_propagation)\n",
        "            self.model.update_prior()\n",
        "\n",
        "            task_end_time = time.time()\n",
        "            print(f\"Task {t} Complete took: {task_end_time - task_start_time}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKoaT6kvMj9d"
      },
      "outputs": [],
      "source": [
        "vcl_trainer = VCLReservoir(in_dim=784, hidden=[100, 100], n_classes=10,\n",
        "single_head=True, lr=1e-3, mc=3, prior_var=1.0,\n",
        "coreset_size=2000, coreset_epochs=30, weighting_scheme=\"uniform\"\n",
        ")\n",
        "\n",
        "permuted_mnist_data = PermutedMNIST(num_tasks=20, seed = 100)\n",
        "vcl_trainer.fit(permuted_mnist_data, epochs=100, batch_size=256)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the run above wasn't exactly the run I used in the paper, so it's slightly different but it's clearly consistent with the results from the paper"
      ],
      "metadata": {
        "id": "HeoKtBJQHqRT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}