{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9s8nwO5EDzi"
      },
      "outputs": [],
      "source": [
        "import math, torch, torch.nn as nn, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np, random, os, time, tarfile, gc\n",
        "import copy\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device\", device)\n",
        "torch.manual_seed(1); random.seed(1); np.random.seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PermutedMNIST:\n",
        "    def __init__(self, num_tasks=10, seed=123):\n",
        "        shuffler = torch.Generator().manual_seed(seed)\n",
        "        self.perms = [torch.randperm(784, generator=shuffler) for _ in range(num_tasks)]\n",
        "        tfm = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "        tr = datasets.MNIST(\"./data\", True,  download=True, transform=tfm)\n",
        "        te = datasets.MNIST(\"./data\", False, download=True, transform=tfm)\n",
        "        x_tr = tr.data.float().view(-1,784)/255.\n",
        "        x_te = te.data.float().view(-1,784)/255.\n",
        "        y_tr = F.one_hot(tr.targets, 10).float()\n",
        "        y_te = F.one_hot(te.targets, 10).float()\n",
        "        self.tasks = [(TensorDataset(x_tr[:, p], y_tr), TensorDataset(x_te[:, p], y_te))for p in self.perms]\n",
        "        self.input_dim = 784\n",
        "        self.n_classes = 10\n",
        "        self.num_tasks = num_tasks\n",
        "    def get_task(self, tid):\n",
        "        return self.tasks[tid]\n",
        "\n",
        "class SplitMNIST:\n",
        "    pairs = [(0,1), (2,3), (4,5), (6,7), (8,9)]\n",
        "    def __init__(self):\n",
        "        tfm = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.view(-1))])\n",
        "        tr = datasets.MNIST(\"./data\", True,  download=True, transform=tfm)\n",
        "        te = datasets.MNIST(\"./data\", False, download=True, transform=tfm)\n",
        "        x_tr = tr.data.float().view(-1,784)/255.\n",
        "        x_te = te.data.float().view(-1,784)/255.\n",
        "        self.tasks=[]\n",
        "        for a, b in self.pairs:\n",
        "            msk_tr = (tr.targets==a)|(tr.targets==b)\n",
        "            msk_te = (te.targets==a)|(te.targets==b)\n",
        "            y_tr = F.one_hot((tr.targets[msk_tr]==b).long(), 2).float()\n",
        "            y_te = F.one_hot((te.targets[msk_te]==b).long(), 2).float()\n",
        "            self.tasks.append((TensorDataset(x_tr[msk_tr], y_tr), TensorDataset(x_te[msk_te], y_te)))\n",
        "        self.input_dim = 784\n",
        "        self.n_classes = 2\n",
        "        self.num_tasks = 5\n",
        "    def get_task(self, tid):\n",
        "        return self.tasks[tid]"
      ],
      "metadata": {
        "id": "C4CJKzJ4EHW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SplitNotMNIST:\n",
        "    # pairs are A/F , B/G , C/H , D/I , E/J: A, B, C, D, E, F, G, H, I, J\n",
        "    _pairs = [(0,5), (1,6), (2,7), (3,8), (4,9)]\n",
        "    def __init__(self, path=\"notMNIST_small.tar.gz\", num_tasks=5, seed=0):\n",
        "        self.num_tasks = num_tasks\n",
        "\n",
        "        base_folder = os.path.splitext(os.path.splitext(path)[0])[0]\n",
        "        if not os.path.isdir(base_folder):\n",
        "            with tarfile.open(path, \"r:gz\") as tar:\n",
        "                tar.extractall()\n",
        "\n",
        "        xs, ys = [], []\n",
        "        char_to_idx = {chr(ord('A')+i): i for i in range(10)}\n",
        "        for char, idx in char_to_idx.items():\n",
        "            folder = os.path.join(base_folder, char)\n",
        "            for fname in os.listdir(folder):\n",
        "                if fname.startswith('.'): continue\n",
        "                path = os.path.join(folder, fname)\n",
        "                try:\n",
        "                    img = Image.open(path).convert(\"L\")\n",
        "                    arr = np.asarray(img, dtype=np.float32)\n",
        "                    arr /= 255.0\n",
        "                    if arr.shape == (28,28):\n",
        "                        xs.append(arr.flatten())\n",
        "                        ys.append(idx)\n",
        "                except Exception:\n",
        "                    pass\n",
        "        xs = torch.tensor(np.stack(xs))\n",
        "        ys = torch.tensor(ys)\n",
        "\n",
        "        # do te/tr div\n",
        "        N = xs.size(0)\n",
        "        perm = torch.randperm(N, generator=torch.Generator().manual_seed(seed))\n",
        "        split = int(0.9*N)\n",
        "        x_tr, y_tr = xs[perm[:split]], ys[perm[:split]]\n",
        "        x_te, y_te = xs[perm[split:]], ys[perm[split:]]\n",
        "        del xs, ys; gc.collect()\n",
        "\n",
        "        self.tasks = []\n",
        "        for tid, pair in enumerate(self._pairs[:num_tasks]):\n",
        "            a,b      = pair\n",
        "            msk_tr   = (y_tr==a)|(y_tr==b)\n",
        "            msk_te   = (y_te==a)|(y_te==b)\n",
        "            map_helper = {a:0, b:1}\n",
        "            ytr_m = torch.tensor([map_helper[l.item()] for l in y_tr[msk_tr]])\n",
        "            yte_m = torch.tensor([map_helper[l.item()] for l in y_te[msk_te]])\n",
        "            ytr_1h = F.one_hot(ytr_m, 2).float()\n",
        "            yte_1h = F.one_hot(yte_m, 2).float()\n",
        "            self.tasks.append((TensorDataset(x_tr[msk_tr], ytr_1h), TensorDataset(x_te[msk_te], yte_1h)))\n",
        "\n",
        "        self.input_dim = 784\n",
        "        self.n_classes = 2\n",
        "\n",
        "    def get_task(self, tid):\n",
        "        return self.tasks[tid]"
      ],
      "metadata": {
        "id": "Bf83E_VhYtXz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(model, loader, head=None):\n",
        "    model.eval()\n",
        "    correct = n = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            preds = model(x, head).argmax(1)\n",
        "            correct += (preds == y.argmax(1)).sum().item()\n",
        "            n += y.size(0)\n",
        "    return correct / n\n"
      ],
      "metadata": {
        "id": "PLB6WKWVEKOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layers, out_dim,\n",
        "                 multi_head=False, num_tasks=10):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        dim = input_dim\n",
        "        for h in hidden_layers:\n",
        "            layers += [nn.Linear(dim, h), nn.ReLU()]\n",
        "            dim = h\n",
        "        self.encoder = nn.Sequential(*layers)\n",
        "\n",
        "        self.multi_head = multi_head\n",
        "        if multi_head:\n",
        "            self.heads = nn.ModuleList([nn.Linear(dim, out_dim) for _ in range(num_tasks)])\n",
        "        else:\n",
        "            self.head = nn.Linear(dim, out_dim)\n",
        "\n",
        "    def forward(self, x, task_id=None):\n",
        "        h = self.encoder(x)\n",
        "        if self.multi_head:\n",
        "            return self.heads[task_id](h)\n",
        "        return self.head(h)"
      ],
      "metadata": {
        "id": "j0UdhF-yEMS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_task(model, reg, loader, task_id,\n",
        "               epochs=20, lr=1e-3, print_every=5):\n",
        "    if hasattr(reg, \"begin_task\"):          # for SI calc\n",
        "        reg.begin_task()\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    model.train()\n",
        "\n",
        "    for ep in range(epochs):\n",
        "        pbar = tqdm(loader, desc=f\"T{task_id} ep{ep+1}\", leave=False)\n",
        "        for step, (x, y) in enumerate(pbar, 1):\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            opt.zero_grad()\n",
        "            logits = model(x, task_id)\n",
        "            ll  = F.cross_entropy(logits, y.argmax(1))\n",
        "            rg  = reg.penalty()\n",
        "            loss = ll + rg\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            if hasattr(reg, \"accumulate_omega\"):   # SI hook\n",
        "                reg.accumulate_omega()\n",
        "\n",
        "            if step % print_every == 0 or step == len(loader):\n",
        "                pbar.set_postfix(ll=f\"{ll.item()}\", reg=f\"{rg.item()}\")\n",
        "        pbar.close()"
      ],
      "metadata": {
        "id": "G9xBOP5LETF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(dataset, hidden=[100, 100], reg_type=\"EWC\",\n",
        "                   lam=100, epochs=20, lr=1e-3, batch=256, multi_head = False):\n",
        "\n",
        "    model = MLP(dataset.input_dim, hidden, dataset.n_classes, multi_head=multi_head, num_tasks=dataset.num_tasks).to(device)\n",
        "\n",
        "    BASELINES = {\"EWC\": EWC, \"LP\": LP, \"SI\": SI, \"LP_fancy\": LP_fancy}\n",
        "    reg = BASELINES[reg_type](model, lam)\n",
        "\n",
        "    acc = torch.zeros(dataset.num_tasks, dataset.num_tasks)\n",
        "\n",
        "    for tid in range(dataset.num_tasks):\n",
        "        train_ds, _ = dataset.get_task(tid)\n",
        "        loader = DataLoader(train_ds, batch)\n",
        "\n",
        "        train_task(model, reg, loader, tid if multi_head else None, epochs=epochs, lr=lr)\n",
        "\n",
        "        reg.update_stats(loader, tid if multi_head else None)\n",
        "\n",
        "        # eval\n",
        "        row = []\n",
        "        for j in range(tid + 1):\n",
        "            _, test_ds = dataset.get_task(j)\n",
        "            test_loader = DataLoader(test_ds, batch)\n",
        "            a = accuracy(model, test_loader, j if multi_head else None)\n",
        "            acc[tid, j] = a\n",
        "            row.append(f\"T{j}:{a}\")\n",
        "\n",
        "        avg = acc[tid, :tid + 1].mean().item()\n",
        "        print(f\"{reg_type} Task {tid}  avg={avg}  |  \" + \" \".join(row))\n",
        "\n",
        "    return acc"
      ],
      "metadata": {
        "id": "SGeVsuGiEl83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EWC:\n",
        "    def __init__(self, model: nn.Module, lam: float = 400.0):\n",
        "        self.model = model.to(device)\n",
        "        self.lam   = lam\n",
        "        self.saved_means   = []\n",
        "        self.saved_fishers = []\n",
        "\n",
        "    def flat_params(self):\n",
        "        return torch.cat([p.view(-1) for p in self.model.parameters()])\n",
        "\n",
        "    # diag Fisher\n",
        "    def diag_fisher(self, loader, task_id=None):\n",
        "        fisher = torch.zeros_like(self.flat_params())\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            self.model.zero_grad()\n",
        "            loss = F.cross_entropy(self.model(x, task_id), y.argmax(1), reduction='sum')\n",
        "            loss.backward()\n",
        "\n",
        "            grads2 = [(p.grad.detach() if p.grad is not None else torch.zeros_like(p)).view(-1) ** 2 for p in self.model.parameters()]\n",
        "            fisher += torch.cat(grads2)\n",
        "\n",
        "        return fisher / len(loader.dataset)\n",
        "\n",
        "    def penalty(self):\n",
        "        if not self.saved_means:\n",
        "            return torch.tensor(0.0, device=device)\n",
        "\n",
        "        theta = self.flat_params()\n",
        "        total = 0.0\n",
        "        for mu, F_diag in zip(self.saved_means, self.saved_fishers):\n",
        "            total += (F_diag * (theta - mu) ** 2).sum()\n",
        "        return 0.5 * self.lam * total\n",
        "\n",
        "    def update_stats(self, loader, task_id=None):\n",
        "        f_diag = self.diag_fisher(loader, task_id).detach()\n",
        "\n",
        "        self.saved_means.append(self.flat_params().detach().clone())\n",
        "        self.saved_fishers.append(f_diag)"
      ],
      "metadata": {
        "id": "JL3kEh6mEvnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SI:\n",
        "    def __init__(self, model, lam=0.1, damping=1e-1):\n",
        "        self.model   = model\n",
        "        self.lam     = lam\n",
        "        self.damping = damping\n",
        "\n",
        "        # state\n",
        "        self.big_omega   = None\n",
        "        self.star_params = None\n",
        "\n",
        "        # task helps\n",
        "        self.prev_params  = None\n",
        "        self.last_params  = None\n",
        "        self.omega_accum  = None\n",
        "\n",
        "    def params(self):\n",
        "        return [p for p in self.model.parameters() if p.requires_grad]\n",
        "\n",
        "    def begin_task(self):\n",
        "        #start of task run this\n",
        "        self.prev_params  = [p.detach().clone() for p in self.params()]\n",
        "        self.last_params  = [p.detach().clone() for p in self.params()]\n",
        "        self.omega_accum  = [torch.zeros_like(p) for p in self.params()]\n",
        "\n",
        "    def accumulate_omega(self):\n",
        "        # after opt run this\n",
        "        for w_acc, p, last in zip(self.omega_accum, self.params(), self.last_params):\n",
        "            if p.grad is not None:\n",
        "                delta = p.detach() - last\n",
        "                w_acc += (-p.grad.detach()) * delta\n",
        "                last.copy_(p.detach())\n",
        "\n",
        "    def update_stats(self, loader, task_id=None):\n",
        "        # debug\n",
        "        if self.prev_params is None: print(\"begin task not working\")\n",
        "\n",
        "        if self.big_omega is None:\n",
        "            self.big_omega = [torch.zeros_like(w) for w in self.omega_accum]\n",
        "\n",
        "        for big_O, w_acc, p, p_start in zip(self.big_omega, self.omega_accum, self.params(), self.prev_params):\n",
        "            delta_total = p.detach() - p_start\n",
        "            update = w_acc / (delta_total.pow(2) + self.damping)\n",
        "            big_O += torch.clamp(update, min=0.0) # let's only allow noneg, makes sense\n",
        "\n",
        "        self.star_params = [p.detach().clone() for p in self.params()]\n",
        "\n",
        "        # reset helpers\n",
        "        self.prev_params  = None\n",
        "        self.last_params  = None\n",
        "        self.omega_accum  = None\n",
        "\n",
        "    def penalty(self):\n",
        "        if self.big_omega is None or self.star_params is None:\n",
        "            return torch.tensor(0.0, device=self.params()[0].device)\n",
        "\n",
        "        total = 0.0\n",
        "        for big_O, p, p_star in zip(self.big_omega, self.params(), self.star_params):\n",
        "            total += (big_O * (p - p_star).pow(2)).sum()\n",
        "        return 0.5 * self.lam * total\n"
      ],
      "metadata": {
        "id": "aROm83OZFmKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LP:\n",
        "    def __init__(self, model, lam=1.0):\n",
        "        self.model, self.lam = model, lam\n",
        "        self.prev_mu = None\n",
        "        self.precision = None\n",
        "\n",
        "    def flat_params(self):\n",
        "        return torch.cat([p.view(-1) for p in self.model.parameters()])\n",
        "\n",
        "    def diag_hessian(self, loader, task_id):\n",
        "        h = torch.zeros_like(torch.cat([p.view(-1) for p in self.model.parameters()]))\n",
        "\n",
        "        for x, y in loader:\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            self.model.zero_grad()\n",
        "\n",
        "            loss = F.cross_entropy(self.model(x, task_id), y.argmax(1), reduction='sum')\n",
        "            loss.backward()\n",
        "\n",
        "            # squared grad\n",
        "            h += torch.cat([(p.grad if p.grad is not None else torch.zeros_like(p)).view(-1).pow(2) for p in self.model.parameters()])\n",
        "\n",
        "        return h / len(loader.dataset)\n",
        "\n",
        "    def update_stats(self, loader, task_id=None):\n",
        "        self.prev_mu = self.flat_params().detach().clone()\n",
        "        hdiag = self.diag_hessian(loader, task_id).detach()\n",
        "        if self.precision is None:\n",
        "            self.precision = hdiag\n",
        "        else:\n",
        "            self.precision += hdiag\n",
        "\n",
        "    def penalty(self):\n",
        "        if self.prev_mu is None:\n",
        "            return torch.tensor(0., device=device)\n",
        "        delta = self.flat_params() - self.prev_mu\n",
        "        return 0.5 * self.lam * (self.precision * delta.pow(2)).sum()"
      ],
      "metadata": {
        "id": "JZeEvVO7N_ir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LP_fancy:\n",
        "    def __init__(self, model, lam=1.0, eps=1e-3):\n",
        "        self.model  = model\n",
        "        self.lam    = lam\n",
        "        self.eps    = eps\n",
        "\n",
        "        # state helpers\n",
        "        self.mu_prev      = None\n",
        "        self.prec_weights = None\n",
        "        self.prec_biases  = None\n",
        "\n",
        "    @staticmethod\n",
        "    def linear_layers(model):\n",
        "        return [m for m in model.modules() if isinstance(m, nn.Linear)]\n",
        "\n",
        "    def collect_activs(self, x, task_id):\n",
        "        activs = []\n",
        "        hooks = [m.register_forward_hook(lambda m,i,o: activs.append(i[0].detach())) for m in self.linear_layers(self.model)]\n",
        "        with torch.no_grad():\n",
        "            self.model(x.to(device)[:1], task_id)\n",
        "        for h in hooks: h.remove()\n",
        "        return activs\n",
        "\n",
        "    def kfac_blocks(self, loader, task_id):\n",
        "        L = len(self.linear_layers(self.model))\n",
        "        Q_sum  = [None]*L\n",
        "        B_sum  = [None]*L\n",
        "        bias_h = [torch.zeros_like(m.bias) for m in self.linear_layers(self.model)]\n",
        "\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            self.model.zero_grad()\n",
        "            loss = F.cross_entropy(self.model(x, task_id), y.argmax(1), reduction='sum')\n",
        "            loss.backward()\n",
        "\n",
        "            activs = self.collect_activs(x, task_id)\n",
        "\n",
        "            for idx, m in enumerate(self.linear_layers(self.model)):\n",
        "\n",
        "                Q = activs[idx].t() @ activs[idx] / activs[idx].size(0)\n",
        "                Q += self.eps * torch.eye(Q.size(0), device=device)   # shift eigenvalues, illcond\n",
        "                Q_sum[idx] = Q if Q_sum[idx] is None else Q_sum[idx] + Q\n",
        "\n",
        "                g = m.weight.grad.detach()\n",
        "                B = (g @ g.t()) / x.size(0)\n",
        "                B += self.eps * torch.eye(B.size(0), device=device)   # shift eigenvalues\n",
        "                B_sum[idx] = B if B_sum[idx] is None else B_sum[idx] + B\n",
        "\n",
        "                bias_h[idx] += (m.bias.grad.detach()**2) / x.size(0)\n",
        "\n",
        "        num_batches = len(loader)\n",
        "        kfac_blocks = [(Q / num_batches, B / num_batches) for Q, B in zip(Q_sum, B_sum)]\n",
        "        bias_blocks = [b / num_batches + self.eps for b in bias_h]  # shift eigenvalues numerical stability\n",
        "\n",
        "        return kfac_blocks, bias_blocks\n",
        "\n",
        "    def penalty(self):\n",
        "        if self.mu_prev is None:\n",
        "            return torch.tensor(0.0, device=next(self.model.parameters()).device)\n",
        "\n",
        "        total = 0.0\n",
        "        for (Q, B), (W_star, b_star), m, bias_prec in zip(self.prec_weights, self.mu_prev, self.linear_layers(self.model), self.prec_biases):\n",
        "\n",
        "            dW = m.weight - W_star\n",
        "            total += (B @ dW @ Q * dW).sum()\n",
        "\n",
        "            db = m.bias - b_star\n",
        "            total += (bias_prec * db.pow(2)).sum()\n",
        "\n",
        "        return 0.5 * self.lam * total\n",
        "\n",
        "    def update_stats(self, loader, task_id=None):\n",
        "        self.mu_prev = [(m.weight.detach().clone(), m.bias.detach().clone()) for m in self.linear_layers(self.model)]\n",
        "\n",
        "        blocks, bias_blocks = self.kfac_blocks(loader, task_id)\n",
        "\n",
        "        if self.prec_weights is None:\n",
        "            self.prec_weights = blocks\n",
        "            self.prec_biases  = bias_blocks\n",
        "            return\n",
        "\n",
        "        for (Q_old, B_old), (Q_new, B_new) in zip(self.prec_weights, blocks):\n",
        "            Q_old += Q_new\n",
        "            B_old += B_new\n",
        "\n",
        "        for i in range(len(self.prec_biases)):\n",
        "            self.prec_biases[i] += bias_blocks[i]\n"
      ],
      "metadata": {
        "id": "B5NvM93QShgE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "perm = PermutedMNIST(num_tasks=10)\n",
        "acc_ewc = run_experiment(perm,\n",
        "                         hidden=[100, 100],\n",
        "                         reg_type=\"EWC\",\n",
        "                         lam=0.45,\n",
        "                         epochs=1,\n",
        "                         batch=256)"
      ],
      "metadata": {
        "id": "VA1XMk42ihDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uVpUhBECZu4V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}